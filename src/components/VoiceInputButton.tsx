import React, { useRef, useState } from "react";

// Google Speech-to-Text APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
const GOOGLE_SPEECH_API_URL = "https://speech.googleapis.com/v1/speech:recognize?key=" + import.meta.env.VITE_GOOGLE_SPEECH_API_KEY;

export type VoiceInputButtonProps = {
  onResult: (text: string) => void;
  className?: string;
  recordSeconds?: number; // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ç§’
};

const VoiceInputButton: React.FC<VoiceInputButtonProps> = ({ onResult, className, recordSeconds = 10 }) => {
  const [recording, setRecording] = useState(false);
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const chunksRef = useRef<Blob[]>([]);
  const streamRef = useRef<MediaStream | null>(null);
  const timeoutRef = useRef<NodeJS.Timeout | null>(null);

  const startRecording = async () => {
    if (recording || loading) return;
    setError(null);
    setRecording(true);
    setLoading(false);
    chunksRef.current = [];

    // APIã‚­ãƒ¼ãƒã‚§ãƒƒã‚¯
    if (!import.meta.env.VITE_GOOGLE_SPEECH_API_KEY) {
      setError("Google Speech-to-Text APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚");
      setRecording(false);
      return;
    }

    // MediaRecorderã‚µãƒãƒ¼ãƒˆãƒã‚§ãƒƒã‚¯
    if (typeof window.MediaRecorder === "undefined") {
      setError("ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯éŸ³å£°éŒ²éŸ³ã«å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã€‚");
      setRecording(false);
      return;
    }

    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      streamRef.current = stream;
      let mimeType = "audio/webm";
      if (!MediaRecorder.isTypeSupported(mimeType)) {
        mimeType = "";
      }
      const mediaRecorder = new MediaRecorder(stream, mimeType ? { mimeType } : undefined);
      mediaRecorderRef.current = mediaRecorder;
      mediaRecorder.ondataavailable = (e) => {
        if (e.data.size > 0) chunksRef.current.push(e.data);
      };
      mediaRecorder.onstop = async () => {
        setRecording(false);
        setLoading(true);
        try {
          const audioBlob = new Blob(chunksRef.current, { type: mimeType || undefined });
          const reader = new FileReader();
          reader.onloadend = async () => {
            const base64Audio = (reader.result as string).split(",")[1];
            const body = {
              config: {
                encoding: "OGG_OPUS",
                sampleRateHertz: 48000,
                languageCode: "ja-JP"
              },
              audio: {
                content: base64Audio
              }
            };
            try {
              const res = await fetch(GOOGLE_SPEECH_API_URL, {
                method: "POST",
                headers: { "Content-Type": "application/json" },
                body: JSON.stringify(body)
              });
              if (!res.ok) {
                const errText = await res.text();
                setError(`APIãƒªã‚¯ã‚¨ã‚¹ãƒˆå¤±æ•—: ${res.status} ${errText}`);
                setLoading(false);
                return;
              }
              const data = await res.json();
              const transcript = data?.results?.[0]?.alternatives?.[0]?.transcript || "";
              if (!transcript) {
                setError("éŸ³å£°èªè­˜çµæœãŒã‚ã‚Šã¾ã›ã‚“");
              }
              onResult(transcript);
            } catch (err: any) {
              setError("éŸ³å£°èªè­˜APIã‚¨ãƒ©ãƒ¼: " + (err?.message || err));
            } finally {
              setLoading(false);
            }
          };
          reader.readAsDataURL(audioBlob);
        } catch (err: any) {
          setError("éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚¨ãƒ©ãƒ¼: " + (err?.message || err));
          setLoading(false);
        }
      };
      mediaRecorder.start();
      timeoutRef.current = setTimeout(() => stopRecording(), recordSeconds * 1000);
    } catch (err: any) {
      setError("ãƒã‚¤ã‚¯ã®åˆ©ç”¨ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“: " + (err?.message || err));
      setRecording(false);
    }
  };

  const stopRecording = () => {
    if (!recording) return;
    if (mediaRecorderRef.current && mediaRecorderRef.current.state === "recording") {
      mediaRecorderRef.current.stop();
    }
    if (streamRef.current) {
      streamRef.current.getTracks().forEach(track => track.stop());
      streamRef.current = null;
    }
    if (timeoutRef.current) {
      clearTimeout(timeoutRef.current);
      timeoutRef.current = null;
    }
    setRecording(false);
  };

  return (
    <div className={className ? className : "ml-2 flex items-center"}>
      {!recording && !loading && (
        <button
          type="button"
          onClick={startRecording}
          className="w-10 h-10 flex items-center justify-center rounded-full bg-primary-500 hover:bg-primary-600 shadow-lg text-white text-2xl transition-all duration-200 focus:outline-none focus:ring-2 focus:ring-primary-400"
          title="ãƒã‚¤ã‚¯ã§å…¥åŠ›"
        >
          <span role="img" aria-label="ãƒã‚¤ã‚¯">ğŸ¤</span>
        </button>
      )}
      {recording && (
        <button
          type="button"
          onClick={stopRecording}
          className="w-10 h-10 flex items-center justify-center rounded-full bg-red-500 text-white text-2xl animate-pulse shadow-lg transition-all duration-200 focus:outline-none focus:ring-2 focus:ring-red-400"
          title="éŒ²éŸ³åœæ­¢"
        >
          <span role="img" aria-label="åœæ­¢">â¹ï¸</span>
        </button>
      )}
      {loading && (
        <span className="ml-2 text-primary-600 animate-pulse text-sm">èªè­˜ä¸­...</span>
      )}
      {error && <div className="text-xs text-red-600 mt-1 ml-2">{error}</div>}
    </div>
  );
};

export default VoiceInputButton; 